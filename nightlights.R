require(readr)
require(dplyr)

require(lubridate)

require(rgdal)
require(raster)

require(sp)
require(rgeos)
require(rworldmap)
require(cleangeo)

library(foreach) #Enables for-each statements to be used for parallel processing
library(doParallel) #Allows for parallel processing using multiple cores


ntLtsIndexUrlViirs <- "https://www.ngdc.noaa.gov/eog/viirs/download_monthly.html"

#6 nightlight tiles named by top-left geo coordinate numbered from left-right & top-bottom
#creates columns as strings. createSpPolysDF converts relevant columns to numeric
nlTiles <- as.data.frame(cbind(id=c(1,2,3,4,5,6), name=c("75N180W","75N060W","75N060E","00N180W","00N060W","00N060E"), minx=c(-180, -60, 60, -180, -60, 60), maxx=c(-60, 60, 180, -60, 60, 180), miny=c(0, 0, 0, -75, -75, -75), maxy=c(75, 75, 75, 0, 0, 0)), stringsAsFactors=F)

#projection system to use
#can we use only one or does it depend on the shapefile loaded?
wgs84 <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"

map <- rworldmap::getMap()
map <- clgeo_Clean(map)

#Set raster directory path
dirRasterOLS <- "./tiles"

#Set directory path
dirRasterVIIRS <- "./tiles"

dirRasterOutput <- "outputrasters"

dirPolygon <- "./polygons"

dirNlData <- "./data"

shpTopLyrName <- "adm0"

createNlTilesSpPolysDF <- function()
{
  #convert nlTiles min/max columns to numeric
  for (cIdx in grep("id|min|max", names(nlTiles))) nlTiles[,cIdx] <- as.numeric(as.character(nlTiles[, cIdx]))
  
  #create the empty obj to hold the data frame of tile PolygonsDataFrams
  tSpPolysDFs <- NULL
  
  #for each row in nlTiles
  for (i in 1:nrow(nlTiles))
  {
    #grab the row containing the tile
    t <- nlTiles[i,]
    
    #convert the tile x,y extents to a matrix
    #format is 2 cols x & y
    tMat <- as.matrix(cbind(rbind(t$minx, t$maxx, t$maxx, t$minx), rbind(t$maxy, t$maxy, t$miny, t$miny)))
    
    #create a Polygon object from the tile extents matrix
    tPoly <- list(Polygon(tMat))
  
    #create a Polygons object with a list of 1 polygon
    tPolys <- Polygons(srl = tPoly, ID = "nltiles")
    
    #create a SpatialPolygons object with a list of 1 list of Polygons
    tSpPolys <- SpatialPolygons(Srl = list(tPolys))
    
    #we assign the CRS at this point (note other objects cannot be assigned CRS)
    projection(tSpPolys) <- CRS(wgs84)
    
    #convert the SpatialPolygons object into a SpatialPolygonsDataFrame
    tSpPolysDF <- as(tSpPolys, "SpatialPolygonsDataFrame")
    
    #append the SPDF into a dataframe of SPDFs
    if (is.null(tSpPolysDFs))
      tSpPolysDFs <- tSpPolysDF
    else
      tSpPolysDFs <- rbind(tSpPolysDFs, tSpPolysDF)
  }
  return (tSpPolysDFs)
}

plotCtryWithTiles <- function(idx)
{
  map <- rworldmap::getMap()
  map <- clgeo_Clean(map)
 
  if (is.numeric(idx))
  {
    if(idx < 0 || idx > length(map@polygons))
    {
      return("Index out of range")
    }
  }
  else
  {
    ctryISO3 <- rwmGetISO3(idx)
    
    #print(ctryISO3)

    if (is.na(ctryISO3) || ctryISO3 == "")
      return("Unknown country")
    
    idx <- which(as.character(map@data$ISO3) == ctryISO3)
  }

  #print (idx)
   
  ctryPolys <- map@polygons[[idx]]
  
  #create a SpatialPolygons object with a list of 1 list of Polygons
  ctrySpPolys <- SpatialPolygons(Srl = list(ctryPolys))
  
  crs(ctrySpPolys) <- CRS(wgs84)
  
  ctrySpPolysDF <- as(ctrySpPolys, "SpatialPolygonsDataFrame")
  
  plot(tSpPolysDFs)
  plot(ctrySpPolysDF, add=TRUE)
  
  #ggplot(tSpPolysDFs, aes(x=long,y=lat))+geom_polygon(col="black", fill="white", alpha=0.5)#+geom_polygon(data=ctrySpPolysDF, alpha=0.5)
  #ggplot(ctrySpPolysDF, aes(x=long,y=lat, group=group))+geom_polygon(col="black", fill="white",alpha=0.5)
  
  #a <- spplot(tSpPolysDFs, main=map@polygons[[idx]]@ID)
  #b <- spplot(ctrySpPolysDF)
  
  #a+as.layer(b)
  
}

mapAllCtryPolyToTiles <- function()
{
  #get list of all country codes
  ctryCodes <- getAllNlCtryCodes()
  
  map <- rworldmap::getMap()
  map <- clgeo_Clean(map)
  
  ctryCodeTiles <- NULL
  
  for (i in 1:length(ctryCodes))
  {
    ctryPolys <- map@polygons[[i]]
    
    #create a SpatialPolygons object with a list of 1 list of Polygons
    ctrySpPolys <- SpatialPolygons(Srl = list(ctryPolys))
    
    crs(ctrySpPolys) <- CRS(wgs84)
    
    ctrySpPolysDF <- as(ctrySpPolys, "SpatialPolygonsDataFrame")
    
    ctryCodeTiles <- rbind(ctryCodeTiles, list(tilesPolygonIntersect(ctrySpPolys)))
  }

  ctryCodeTiles <- as.data.frame(cbind(code = as.character(map@data$ISO3), tiles = ctryCodeTiles))
  
  names(ctryCodeTiles) <- c("code", "tiles")
  
  ctryCodeTiles$code <- as.character(ctryCodeTiles$code)
  
  #plot(tSpPolysDFs, add=TRUE)
  #plot(ctrySpPolysDF, add=TRUE)
  #
    
  return(ctryCodeTiles)
}

getTilesCtryIntersect <- function(ctryCode)
{
  ctryISO3 <- rwmGetISO3(ctryCode)
  
  #print(ctryISO3)
  
  if (is.na(ctryISO3) || ctryISO3 == "")
    return("Unknown country")
  
  idx <- which(map@data$ISO3 == ctryISO3)
  
  ctryCodeTiles <- NULL
  
  ctryPolys <- map@polygons[[idx]]
  
  #create a SpatialPolygons object with a list of 1 list of Polygons
  ctrySpPolys <- SpatialPolygons(Srl = list(ctryPolys))
  
  crs(ctrySpPolys) <- CRS(wgs84)
  
  ctrySpPolysDF <<- as(ctrySpPolys, "SpatialPolygonsDataFrame")
  
  ctryCodeTiles <- tilesPolygonIntersect(ctrySpPolys)
  
  #plot(tSpPolysDFs, add=TRUE)
  #plot(ctrySpPolysDF, add=TRUE)
  
  return (ctryCodeTiles)
}

tileName2Idx <- function(tileName)
{
  return (which(nlTiles$name == tileName))
}

tileIdx2Name <- function(tileNum)
{
  return (nlTiles[tileNum, "name"])
}

tilesPolygonIntersect <- function(shp_polygon)
{
  #given a polygon this function returns a list of the names of the viirs tiles
  #that it intersects with
  #Input: a Spatial Polygon e.g. from a loaded shapefile
  #Output: a character vector of tile names as given in the nlTiles dataframe
  
  #init list to hold tile indices
  tileIdx <- NULL
  
  #loop through the 6 tile rows in our SpatialPolygonsDataFrame
  for (i in 1:nrow(tSpPolysDFs))
  {
    #check whether the polygon intersects with the current tile
    tileIdx[i] <- gIntersects(tSpPolysDFs[i,], shp_polygon)
  }

  #return a list of tiles that intersected with the SpatialPolygon
  return (nlTiles[tileIdx, "name"])
}


getNtLts <- function(inputYear)
{
  #dmsp/ols data from 1992-2013
  #snpp/viirs data from 2014 - present
  
  if (inputYear < 1992 || inputYear > year(now()))
    return ("Invalid year")
  
  if (inputYear < 2014)
    print("Downloading from DMSP/OLS")
  else
    print("Downloading from SNPP/VIIRS")
}

getNtLtsUrlViirs <- function(inYear, inMonth, inTile)
{
  inYear <- as.character(inYear)
   
  nMonth <- as.character(inMonth)

  #Function to return the url of the file to download given the year, month, and nlTile index
  #nlTile is a global list
  
  #the page that lists all available nightlight files
  ntLtsPageHtml <- "https://www.ngdc.noaa.gov/eog/viirs/download_mon_mos_iframe.html"
  
  #the local name of the file once downloaded
  ntLtsPageLocalName <- "ntltspageviirs.html"

  #if the file does not exist or is older than a week download it afresh
  #not working. download.file does not seem to update mtime
  if (!file.exists(ntLtsPageLocalName) || (date(now()) - date(file.mtime(ntLtsPageLocalName)) > as.difftime(period("1 day"))))
  {
    download.file(url = ntLtsPageHtml, destfile = ntLtsPageLocalName, method = "wget")
  }
  #else
  #  print(paste0(ntLtsPageHtml, " already downloaded"))
  
  #read in the html page
  ntLtsPage <- readr::read_lines(ntLtsPageLocalName)
  
  #search for a line containing the patterns that make the files unique i.e.
  #1. SVDNB_npp_20121001 - year+month+01
  #2. vcmcfg - for file with intensity as opposed to cloud-free counts (vcmslcfg)
  #sample url: https://data.ngdc.noaa.gov/instruments/remote-sensing/passive/spectrometers-radiometers/imaging/viirs/dnb_composites/v10//201210/vcmcfg/SVDNB_npp_20121001-20121031_75N180W_vcmcfg_v10_c201602051401.tgz
  
  #create the pattern
  ntLtsPageRgxp <- paste0("SVDNB_npp_", inYear, inMonth, "01.*", nlTiles[inTile,"name"], ".*vcmcfg")
  
  #search for the pattern in the page
  ntLtsPageHtml <- ntLtsPage[grep(pattern = ntLtsPageRgxp, x=ntLtsPage)]
  
  #split the output on quotes since this url is of the form ...<a href="URL"download> ...
  #the url is in the second position
  ntLtsPageUrl <- unlist(strsplit(ntLtsPageHtml, '"'))[2]

  #****NOTE: temp for testing using local download****
  #
  #fname <- stringr::str_extract(ntLtsPageUrl, "SVDNB.*.tgz")
  #ntLtsPageUrl <- paste0("http://localhost/", fname)
  #
  #****DELETE WHEN DONE****
  
  return (ntLtsPageUrl)
}

getNtLtsZipLcllNameVIIRS <- function(nlYear, nlMonth, tileNum)
{
  return (paste0(dirRasterVIIRS, "/viirs_", nlYear, "_", nlMonth, "_", tileIdx2Name(tileNum), ".tgz"))
}

getNtLtsTifLcllNameVIIRS <- function(nlYear, nlMonth, tileNum)
{
  return (paste0(dirRasterVIIRS, "/viirs_", nlYear, "_", nlMonth, "_", tileIdx2Name(tileNum), ".tif"))
}

getNtLtsViirs <- function(nlYear, nlMonth, tileNum)
{
  rsltDnld <- NA
  
  ntLtsZipLocalNameVIIRS <- getNtLtsZipLcllNameVIIRS(nlYear, nlMonth, tileNum)
  
  if (!file.exists(ntLtsZipLocalNameVIIRS))
  {
    ntLtsFileUrl <- getNtLtsUrlViirs(nlYear, nlMonth, tileNum)
    
    rsltDnld <- download.file(ntLtsFileUrl, ntLtsZipLocalNameVIIRS, mode = "wb", method = "wget", extra = "-c")
  }
  else
  {
    #if the file is found we can return positive? Probably not unless there's an overwrite option
    #for our purposes return true
    print("File exists, set Overwrite = TRUE to overwrite")
    
    rsltDnld <- 0
  }
  
  if (rsltDnld == 0)
  {
    message("Extracting ", ntLtsZipLocalNameVIIRS, " ", base::date())
    
    if (!file.exists(getNtLtsTifLcllNameVIIRS(nlYear, nlMonth, tileNum)))
    {
      message("Getting list of files in ", ntLtsZipLocalNameVIIRS, " ", base::date())
      
      tgzFileList <- untar(ntLtsZipLocalNameVIIRS, list = TRUE)
      #tgz_file_list <- stringr::str_replace(tgz_file_list,"./","")
      
      if (is.null(tgzFileList))
      {
        message("Error extracting file list. ")
        
        return (-1)
      }
      
      tgzAvgRadFilename <- tgzFileList[grep("svdnb.*.avg_rade9.tif$",tgzFileList, ignore.case = T)]
      
      message("Decompressing ", tgzAvgRadFilename, " ", base::date())
      
      if(!file.exists(getNtLtsTifLcllNameVIIRS(nlYear, nlMonth, tileNum)))
      {
        untar(ntLtsZipLocalNameVIIRS, files = tgzAvgRadFilename, exdir = dirRasterVIIRS)
      
        file.rename(paste0(dirRasterVIIRS,"/",tgzAvgRadFilename), getNtLtsTifLcllNameVIIRS(nlYear, nlMonth, tileNum))
      }
    }
    else
    {
      message("TGZ file found")
    }
  }
  
  return (rsltDnld == 0)
}

masq_viirs <- function(shp, rast, i)
{
  #based on masq function from https://commercedataservice.github.io/tutorial_viirs_part1/
  #slightly modified to use faster masking method by converting the raster to vector
  #Extract one polygon based on index value i
  polygon <- shp[i,] #extract one polygon
  extent <- extent(polygon) #extract the polygon extent 
  
  #Raster extract
  outer <- crop(rast, extent) #extract raster by polygon extent
  #inner <- mask(outer,polygon) #keeps values from raster extract that are within polygon
  inner <- rasterize(polygon, outer, mask=TRUE) #crops to polygon edge & converts to raster
  
  #Convert cropped raster into a vector
  #Specify coordinates
  coords <- expand.grid(seq(extent@xmin,extent@xmax,(extent@xmax-extent@xmin)/(ncol(inner)-1)),
                        seq(extent@ymin,extent@ymax,(extent@ymax-extent@ymin)/(nrow(inner)-1)))
  #Convert raster into vector
  data <- as.vector(inner)
  
  ##THIS SECTION NEEDS WORK. confirm error and NA values and handling of the same
  
  #data <- data[!is.na(data)] #keep non-NA values only ... shall this affect mask values?
  #data[is.na(data)] <- 0
  data[data < 0] <- 0 #any negative values are either recording problems or error values as per: 
  
  return(data) 
}

getNtLtsUrlOls <- function(inYear, inTile)
{
  inYear <- as.character(inYear)
  
  #Function to return the url of the file to download given the year, month, and nlTile index
  #nlTile is a global list
  
  ntLtsBaseUrl <- "https://www.ngdc.noaa.gov/"
  
  #the page that lists all available nightlight files
  ntLtsPageHtml <- "https://www.ngdc.noaa.gov/eog/dmsp/downloadV4composites.html"
  
  #the local name of the file once downloaded
  ntLtsPageLocalName <- "ntltspageols.html"
  
  #if the file does not exist or is older than a week download it afresh
  #not working. download.file does not seem to update mtime
  if (!file.exists(ntLtsPageLocalName) || (date(now()) - date(file.mtime(ntLtsPageLocalName)) > as.difftime(period("1 day"))))
  {
    download.file(url = ntLtsPageHtml, destfile = ntLtsPageLocalName, method = "wget", extra = "-N")
  }
  #else
  #  print(paste0(ntLtsPageHtml, " already downloaded"))
  
  #read in the html page
  ntLtsPage <- readr::read_lines(ntLtsPageLocalName)
  
  #search for a line containing the patterns that make the files unique i.e.
  #1. SVDNB_npp_20121001 - year+month+01
  #2. vcmcfg - for file with intensity as opposed to cloud-free counts (vcmslcfg)
  #sample url: https://data.ngdc.noaa.gov/instruments/remote-sensing/passive/spectrometers-radiometers/imaging/viirs/dnb_composites/v10//201210/vcmcfg/SVDNB_npp_20121001-20121031_75N180W_vcmcfg_v10_c201602051401.tgz
  
  #create the pattern
  ntLtsPageRgxp <- paste0("F.*.", inYear,".*\\.tar")
  
  #search for the pattern in the page
  ntLtsPageHtml <- ntLtsPage[grep(pattern = ntLtsPageRgxp, x=ntLtsPage)]
  
  #split the output on quotes since this url is of the form ...<a href="URL"download> ...
  #the url is in the second position
  ntLtsPageUrl <- unlist(strsplit(ntLtsPageHtml, '"'))
  
  ntLtsPageUrl <- ntLtsPageUrl[grep("v4composite", ntLtsPageUrl)]
  
  ntLtsPageUrl <- unlist(lapply(ntLtsPageUrl, FUN=function(x) paste0(ntLtsBaseUrl, x)))
  
  #****NOTE: temp for testing using local download****
  #
  #fname <- stringr::str_extract(ntLtsPageUrl, "SVDNB.*.tgz")
  #ntLtsPageUrl <- paste0("http://localhost/", fname)
  #
  #****DELETE WHEN DONE****
  
  return (ntLtsPageUrl)
}

getNtLtsZipLcllNameOLS <- function(nlYear, tileNum)
{
  return (paste0(dirRasterOLS, "/ols_", nlYear, "_", tileNum, ".tgz"))
}

getNtLtsTifLcllNameOLS <- function(nlYear, tileNum)
{
  return (paste0(dirRasterOLS, "/ols_", nlYear, "_", tileNum, ".tif"))
}

getNtLtsOLS <- function(nlYear, tileNum)
{
  rsltDnld <- NA
  
  ntLtsZipLocalName <- getNtLtsZipLcllNameOLS(nlYear, tileNum)
  
  if (!file.exists(ntLtsZipLocalNameOLS))
  {
    ntLtsFileUrl <- getNtLtsUrlOLS(nlYear)
    
    rsltDnld <- download.file(ntLtsFileUrl, ntLtsZipLocalName, mode = "wb", method = "wget", extra = "-c")
  }
  else
  {
    #if the file is found we can return positive? Probably not unless there's an overwrite option
    #for our purposes return true
    print("File exists, set Overwrite = TRUE to overwrite")
    
    rsltDnld <- 0
  }
  
  if (rsltDnld == 0)
  {
    message("Extracting ", ntLtsZipLocalName, " ", base::date())
    
    if (!file.exists(getNtLtsTifLcllNameOLS(nlYear, nlMonth, tileNum)))
    {
      message("Getting list of files in ", ntLtsZipLocalName, " ", base::date())
      
      tgzFileList <- untar(ntLtsZipLocalName, list = TRUE)
      #tgz_file_list <- stringr::str_replace(tgz_file_list,"./","")
      
      if (is.null(tgzFileList))
      {
        message("Error extracting file list. ")
        
        return (-1)
      }
      
      tgzAvgRadFilename <- tgzFileList[grep("svdnb.*.avg_rade9.tif$",tgzFileList, ignore.case = T)]
      
      message("Decompressing ", tgzAvgRadFilename, " ", base::date())
      
      if(!file.exists(getNtLtsTifLcllNameVIIRS(nlYear, nlMonth, tileNum)))
      {
        untar(ntLtsZipLocalName, files = tgzAvgRadFilename, exdir = dirRasterOLS)
        
        file.rename(paste0(dirRasterOLS,"/",tgzAvgRadFilename), getNtLtsTifLcllNameOLS(nlYear, tileNum))
      }
    }
    else
    {
      message("TGZ file found")
    }
  }
  
  return (rsltDnld == 0)
}

masq_ols <- function(shp, rast, i)
{
  #based on masq function from https://commercedataservice.github.io/tutorial_viirs_part1/
  #Extract one polygon based on index value i
  polygon <- shp[i,] #extract one polygon
  extent <- extent(polygon) #extract the polygon extent 
  
  #Raster extract
  outer <- crop(rast, extent) #extract raster by polygon extent
  #inner <- mask(outer,polygon) #keeps values from raster extract that are within polygon
  inner <- rasterize(polygon, outer, mask=TRUE) #crops to polygon edge & converts to raster
  
  #Convert cropped raster into a vector
  #Specify coordinates
  coords <- expand.grid(seq(extent@xmin,extent@xmax,(extent@xmax-extent@xmin)/(ncol(inner)-1)),
                        seq(extent@ymin,extent@ymax,(extent@ymax-extent@ymin)/(nrow(inner)-1)))
  #Convert raster into vector
  data <- as.vector(inner)
  
  ##THIS SECTION NEEDS WORK. confirm error and NA values and handling of the same
  #keep non-NA values only?
  #data <- data[!is.na(data)]
  
  #in DMSP-OLS 255 == NA
  data[which(data == 255)] <- NA
  
  #non-negative values are errors replace with 0?
  data[data < 0] <- 0
  
  return(data)
}

ctryNameToCode <- function(ctryName)
{
  return (rworldmap::rwmGetISO3(ctryName))
}

ctryCodeToNAME <- function(ctryCode)
{
  return(rworldmap::isoToName(ctryCode))
}

processNLCountryOls <- function(cntryCode, nlYear)
{
  message("processNLCountryOLS: ")
  
  ctryPoly <- readOGR(getPolyFnamePath(ctryCode), getCtryShpLowestLyrName(ctryCode))
  
  ctryExtent <- extent(ctryPoly)
  
  projection(ctryPoly) <- CRS(wgs84)
  
  nlYear <- substr(nlYearMonth, 1, 4)
  
  nlMonth <- substr(nlYearMonth, 5, 6)
  
  if (existsCtryNlDataFile(ctryCode))
  {
    ctryNlDataDF <- read.csv(getCtryNlDataFnamePath(ctryCode),header = TRUE,sep = ",")
    
    existingDataCols <- names(ctryNlDataDF)
    
    existingYears <- existingDataCols[grep("^NL_[:alphanum:]*", existingDataCols)]
    
    existingYears <- stringr::str_replace(existingYears, "NL_OLS_", "")
    
    if(nlYear %in% existingYears)
    {
      message("Data exists for ", ctryCode, " ", nlYear)
      
      return(-1)
    }
  } else
  {
    #get the list of admin levels in the polygon shapefile
    ctryPolyAdmLevels <- getCtryPolyAdmLevelNames(ctryCode)
    
    #conver to lower case for consistency
    ctryPolyAdmLevels$name <- tolower(ctryPolyAdmLevels$name) 
    
    #the number of admin levels
    nLyrs <- nrow(ctryPolyAdmLevels)
    
    #the repeat pattern required to create columns in the format 1,1,2,2,3,3 ...
    #for col names: admlevel1_id, admlevel1_name, ..., admleveN_id, admlevelN_name
    #and polygon data col names: ID_1, NAME_1, ..., ID_N, NAME_N
    nums <- c(paste(1:nLyrs,1:nLyrs))
    
    nums <- unlist(strsplit(paste(nums, collapse = " "), " "))
    
    ctryPolyAdmCols <- paste(c("ID_", "NAME_"), nums, sep="")
    
    #pull the ID_ and NAME_ cols from layer1 to lowest layer (layer0 has country code not req'd)
    ctryNlDataDF <-ctryPoly@data[,eval(ctryPolyAdmCols)]
    
    #add the area as reported by the polygon shapefile as a convenience
    areas <- area(ctryPoly)
    
    #we add the country code to ensure even a renamed file is identifiable
    #repeat ctryCode for each row in the polygon. equiv of picking layer0
    ctryCodeCol <- rep(ctryCode, nrow(ctryNlDataDF))
    
    #combine the columns
    ctryNlDataDF <- cbind(ctryCodeCol, ctryNlDataDF, areas)
    
    ctryPolyColNames <- paste(ctryPolyAdmLevels[nums, "name"], c("_id", "_name"), sep="")
    
    #add the country_code and area columns to the dataframe
    ctryPolyColNames <- c("country_code", ctryPolyColNames, "area_sq_km")
    
    names(ctryNlDataDF) <- ctryPolyColNames
  }
  
  if(!file.exists(getCtryRasterOutputFname(ctryCode, nlYearMonth)))
  {
    message("Begin processing ", nlYearMonth, " ", base::date())
    
    message("Reading in the rasters " , base::date())
    
    tileList <- getCtryCodeTileList(ctryCode)
    
    ctryRastCropped <- NULL
    
    for (tile in tileList)
    {
      satYear <- substr(tar_fl,1,7)
      
      message("Extracting ", tar_fl, " ", base::date())
      
      message("Getting list of files in ", tar_fl, " ", base::date())
      #get a list of files in the tar archive
      tar_file_list <- untar(tar_fl, list = TRUE)
      
      #get the nightlight data filename
      #the nightlight data filename has the format "web.avg_vis.tif.gz"
      #    tgz_file <- tar_file_list[grep(".*web\\.avg_vis\\.tif\\.gz$",tar_file_list, ignore.case = T)]
      tgz_file <- tar_file_list[grep(".*stable_lights\\.avg_vis\\.tif\\.gz$",tar_file_list, ignore.case = T)]
      
      #extract the nightlight data file
      untar(tar_fl, tgz_file)
      
      #the tif has the same name as the compressed file without the .gz
      tif_file <- str_replace(tgz_file, ".gz", "")
      
      message("Decompressing ", tgz_file, " ", base::date())
      
      gunzip(tgz_file)
      #no need to delete the gz since gunzip deletes the compressed version
    }
  
    #for()
    {
      message("Reading in the raster " , base::date())
      rastGlobal <- raster(tif_file)
      
      projection(rastGlobal) <- CRS(wgs84)
      
      message("Cropping the raster ", base::date())
      ctryRast <- crop(rastGlobal, ctryPoly)
      
      message("Releasing the raster variables")
      rm(rastGlobal)
      
      message("Deleting the raster files ", base::date())
      
      unlink(tif_file)
      
      #merge rasters
    }
    
    #write merged raster
  }
  else
  {
    #read in the raster  
  }
    
  gc()
  
  message("Masking the merged raster ", base::date())
  #rast_ke <- mask(rast_ke, ke_shp_ward)
  rast_ke <- rasterize(ctryPoly, rast_ke, mask=TRUE) #crops to polygon edge & converts to raster
  
  message("Writing the merged raster to disk ", base::date())
  writeRaster(x = rast_ke, filename = paste0(dirRasterOutput, nlYear, ".tif"), overwrite=TRUE)
  
  registerDoParallel(cores=2)
  
  message("Begin extracting the data from the merged raster ", base::date())
  sumAvgRad <- foreach(i=1:nrow(ctryPoly@data), .combine=rbind) %dopar% 
  {
    #message("Extracting data from polygon " , i, " ", base::date())
    dat <- masq(ctryPoly, rast_ke,i)
    
    #message("Calculating the mean of polygon ", i, " ", base::date())
    
    #calculate and return the sum of the mean of all the pixels
    data.frame(mean = sum(dat, na.rm=TRUE))
  }
  
  #merge the calculated means for the polygon as a new column
  ctryNlDataDF <- cbind(extctryNlDract, sumAvgRad)
  
  #name the new column with the yearmonth of the data
  names(extract)[ncol(extract)] <- paste0("NL_OLS_", nlYear)
  
  message("DONE processing ", nlYear, " ", base::date())

  message("COMPLETE. Writing data to disk")
  write.table(extract, getCtryNlDataFname(ctryCode), row.names= F, sep = ",")
    
}

processNLCountriesViirs <- function(ctryCodes, nlYearMonth)
{
  #Download all tiles
  #getNtLtsViirs()
  
  for (nlCtryCode in nlCtryCodes)
    processNLCountryVIIRS(ctryCode, nlYearMonth)
}

ctryShpLyrName2Num <- function(layerName)
{
  return(as.numeric(gsub("[^[:digit:]]", "", layerName)))
}

processNLCountryVIIRS <- function(ctryCode, nlYearMonth)
{
  message("processNLCountryVIIRS: ")
  
  ctryPoly <- readOGR(getPolyFnamePath(ctryCode), getCtryShpLowestLyrName(ctryCode))
  
  ctryExtent <- extent(ctryPoly)
  
  projection(ctryPoly) <- CRS(wgs84)
  
  nlYear <- substr(nlYearMonth, 1, 4)
  
  nlMonth <- substr(nlYearMonth, 5, 6)
  
  if (existsCtryNlDataFile(ctryCode))
  {
    ctryNlDataDF <- read.csv(getCtryNlDataFnamePath(ctryCode),header = TRUE,sep = ",")
    
    existingDataCols <- names(ctryNlDataDF)
    
    existingYearMonths <- existingDataCols[grep("^NL_[:alphanum:]*", existingDataCols)]
    
    existingYearMonths <- stringr::str_replace(existingYearMonths, "NL_VIIRS_", "")
    
    if(nlYearMonth %in% existingYearMonths)
    {
      message("Data exists for ", ctryCode, " ", nlYearMonth)
      
      return(-1)
    }
  } else
  {
    #get the list of admin levels in the polygon shapefile
    ctryPolyAdmLevels <- getCtryPolyAdmLevelNames(ctryCode)
    
    #conver to lower case for consistency
    ctryPolyAdmLevels$name <- tolower(ctryPolyAdmLevels$name) 
    
    #the number of admin levels
    nLyrs <- nrow(ctryPolyAdmLevels)
    
    #the repeat pattern required to create columns in the format 1,1,2,2,3,3 ...
    #for col names: admlevel1_id, admlevel1_name, ..., admleveN_id, admlevelN_name
    #and polygon data col names: ID_1, NAME_1, ..., ID_N, NAME_N
    nums <- c(paste(1:nLyrs,1:nLyrs))
    
    nums <- unlist(strsplit(paste(nums, collapse = " "), " "))
    
    ctryPolyAdmCols <- paste(c("ID_", "NAME_"), nums, sep="")
    
    #pull the ID_ and NAME_ cols from layer1 to lowest layer (layer0 has country code not req'd)
    ctryNlDataDF <-ctryPoly@data[,eval(ctryPolyAdmCols)]
    
    #add the area as reported by the polygon shapefile as a convenience
    areas <- area(ctryPoly)
    
    #we add the country code to ensure even a renamed file is identifiable
    #repeat ctryCode for each row in the polygon. equiv of picking layer0
    ctryCodeCol <- rep(ctryCode, nrow(ctryNlDataDF))
  
    #combine the columns
    ctryNlDataDF <- cbind(ctryCodeCol, ctryNlDataDF, areas)
    
    ctryPolyColNames <- paste(ctryPolyAdmLevels[nums, "name"], c("_id", "_name"), sep="")
    
    #add the country_code and area columns to the dataframe
    ctryPolyColNames <- c("country_code", ctryPolyColNames, "area_sq_km")
    
    names(ctryNlDataDF) <- ctryPolyColNames
  }

  if(!file.exists(getCtryRasterOutputFname(ctryCode, nlYearMonth)))
  {
    message("Begin processing ", nlYearMonth, " ", base::date())
  
    message("Reading in the rasters " , base::date())

    tileList <- getCtryCodeTileList(ctryCode)
    
    ctryRastCropped <- NULL
    
    for (tile in tileList)
    {
      rastFilename <- getNtLtsTifLcllNameVIIRS(nlYear, nlMonth, tileName2Idx(tile))
  
      rastTile <- raster(rastFilename)
      
      projection(rastTile) <- CRS(wgs84)
    
      message("Cropping the rasters ", base::date())
  
      tempCrop <- crop(rastTile, ctryPoly)
      
      if(is.null(ctryRastCropped))
      {
        ctryRastCropped <- tempCrop
      }
      else
      {
        ctryRastMerged <- ctryRastCropped
        
        ctryRastCropped <- NULL
        
        ctryRastCropped <- merge(ctryRastMerged, tempCrop)
      }
      
      rm(tempCrop)
    }
    
    gc()
  
    message("Masking the merged raster ", base::date())
    
    ctryRastCropped <- rasterize(ctryPoly, ctryRastCropped, mask=TRUE) #crops to polygon edge & converts to raster
    
    message("Deleting the component rasters ", base::date())
    
    message("Writing the merged raster to disk ", base::date())
    
    writeRaster(x = ctryRastCropped, filename = getCtryRasterOutputFname(ctryCode,nlYearMonth), overwrite=TRUE)
  }
  else
  {
    rastFilename <- getCtryRasterOutputFname(ctryCode, nlYearMonth)
    
    ctryRastCropped <- raster(rastFilename)
    
    crs(ctryRastCropped) <- CRS(wgs84)
  }
  
  registerDoParallel(cores=2)
  
  message("Begin extracting the data from the merged raster ", base::date())
  
  sumAvgRad <- foreach(i=1:nrow(ctryPoly@data), .combine=rbind) %dopar% 
  {
    message("Extracting data from polygon " , i, " ", base::date())
    dat <- masq_viirs(ctryPoly, ctryRastCropped, i)
    
    message("Calculating the mean of polygon ", i, " ", base::date())
    
    #calculate and return the mean of all the pixels
    data.frame(mean = sum(dat, na.rm=TRUE))
  }
  
  #merge the calculated means for the polygon as a new column
  ctryNlDataDF <- cbind(ctryNlDataDF, sumAvgRad)
  
  #name the new column with the yearmonth of the data
  names(ctryNlDataDF)[ncol(ctryNlDataDF)] <- paste0("NL_VIIRS_", nlYearMonth)
  
  cols <- names(ctryNlDataDF)
  
  nlDataColIdx <- grep("^NL_", cols)
  
  nlDataColNames <- cols[nlDataColIdx]
  
  nlDataColNames <- nlDataColNames[order(nlDataColNames)]
  
  newNlDataColNames <- c(cols[-nlDataColIdx], nlDataColNames)
  
  #rearrange NL columns
  ctryNlDataDF <- ctryNlDataDF[,newNlDataColNames]
  
  message("DONE processing ", ctryCode, " ", nlYearMonth, " ", base::date())
  
  message("COMPLETE. Writing data to disk")
  write.table(ctryNlDataDF, getCtryNlDataFnamePath(ctryCode), row.names= F, sep = ",")
}

getCtryRasterOutputFname <- function(ctryCode, nlYearMonth)
{
  return (paste0(dirRasterOutput, "/",ctryCode, "_", nlYearMonth,".tif"))
}

getCtryPolyUrl <- function(ctryCode)
{
  #Sample url: http://biogeo.ucdavis.edu/data/gadm2.8/shp/AFG_adm_shp.zip
  basePolyUrl <- "http://biogeo.ucdavis.edu/data/gadm2.8/shp/"
  
  return (paste0(basePolyUrl, ctryCode, "_adm_shp.zip"))
}

getCtryNlDataFname <- function(ctryCode)
{
  return (paste0(ctryCode, "_NLData.csv"))
}

getCtryNlDataFnamePath <- function(ctryCode)
{
  return (paste0(dirNlData, "/", getCtryNlDataFname(ctryCode)))
}

existsCtryNlDataFile <- function(ctryCode)
{
  #for polygons look for shapefile dir
  return(file.exists(getCtryNlDataFnamePath(ctryCode)))
}

polyFnamePathExists <- function(ctryCode)
{
  #for polygons look for shapefile dir
  return(dir.exists(getPolyFnamePath(ctryCode)))
}

polyFnameZipExists <- function(ctryCode)
{
  return(file.exists(getPolyFnameZip(ctryCode)))
}

getCtryShpLyrName <- function(ctryCode, lyrNum)
{
  return(paste0(ctryCode, "_adm", lyrNum))
}

getCtryShpLowestLyrName <- function(ctryCode)
{
  layers <- ogrListLayers(getPolyFnamePath(ctryCode))
  
  admLayers <- layers[grep("adm", layers)]
  
  admLayerNums <- gsub("[^[:digit:]]", "", admLayers)
  
  lowestAdmLyrName <- admLayers[order(as.numeric(admLayerNums),decreasing = T)][1]
  
  return(lowestAdmLyrName)
}

getCtryPolyAdmLevelNames <- function(ctryCode)
{
  lowestLayer <- getCtryShpLowestLyrName(ctryCode)
  
  numLayers <- ctryShpLyrName2Num(lowestLayer)
  
  admLevels <- NULL
  
  for (lyrNum in 1:numLayers)
  {
    lyrPoly <- readOGR(getPolyFnamePath(ctryCode), getCtryShpLyrName(ctryCode, lyrNum))
    
    lvlTypeName <- paste0("TYPE_",lyrNum)
    
    lvlName <- unlist(lyrPoly@data[1,eval(lvlTypeName)])
    
    admLevels <- rbind(admLevels, as.character(lvlName))
  }
  
  admLevels <- as.data.frame(cbind(1:numLayers, admLevels))
  
  names(admLevels) <- c("id", "name")
  
  return (admLevels)  
}

dnldCtryPoly <- function(ctryCode)
{
  fullPolyUrl <- getCtryPolyUrl(ctryCode)
  
  result <- NULL
  
  #if the path doesn't exist
  if (!polyFnamePathExists(ctryCode))
  {
    if (!polyFnameZipExists(ctryCode))
    {
      if(download.file(url = getCtryPolyUrl(ctryCode), destfile = getPolyFnameZip(ctryCode), method = "wget", mode = "wb", extra = "-c") == 0)
      {
        result <- unzip(getPolyFnameZip(ctryCode), exdir = getPolyFnamePath(ctryCode))
      }
    }else
    {
      result <- unzip(getPolyFnameZip(ctryCode), exdir = getPolyFnamePath(ctryCode))
    }
  }
  else
  {
    message("Polygon already exists")
  }
  
  return (!is.null(result))
}

getAllNlYears <- function()
{
  return (paste(1992:year(now()), c(paste("0",1:9, sep= ""),10:12), sep=""))
}

getAllNlCtryCodes <- function()
{
  #rworldmap has more country codes in countryRegions$ISO3 than in the map itself
  #select ctryCodes from the map data itself
  map <- rworldmap::getMap()

  #some polygons have problems. use cleangeo package to rectify
  map <- clgeo_Clean(map)
  
  return (as.character(map@data$ISO3))
}

getNlType <- function(nlYear)
{
  if (nlYear < 1992 || nlYear > year(now()))
    return(NA)

  if (nlYear > 1992 && nlYear < 2014)
    return("OLS")
  else
    return("VIIRS")
}

getPolyFname <- function(ctryCode)
{
  #format of shapefiles is CTR_adm_shp e.g. KEN_adm_shp
  polyFname <- paste0(ctryCode, "_adm_shp")
  
  return (polyFname)
}

getPolyFnamePath <- function(ctryCode)
{
  #check for the shapefile directory created with 
  #format of shapefiles is CTR_adm_shp e.g. KEN_adm_shp
  polyFnamePath <- paste0(dirPolygon, "/", getPolyFname(ctryCode))

  return (polyFnamePath)
}

getPolyFnameZip <- function(ctryCode)
{
  #format of shapefiles is CTR_adm_shp e.g. KEN_adm_shp
  polyFname <- paste0(getPolyFnamePath(ctryCode),".zip")
  
  return (polyFname)
}

getNlYearMonthTilesOLS <- function(nlYearMonth, tileList)
{
  success <- TRUE
  
  for (tile in tileList)
  {
    nlYear <- substr(nlYearMonth, 1, 4)
    
    nlMonth <- substr(nlYearMonth, 5, 6)
    
    nlTile <- 
    
    #download tile
    success <- success && getNtLtsOls(nlYear, nlMonth, nlTile)
  }
  
  return (success)
}

getNlYearMonthTilesVIIRS <- function(nlYearMonth, tileList)
{
  success <- TRUE
  
  #ensure we have all required tiles
  for (tile in tileList)
  { 
    nlYear <- substr(nlYearMonth, 1, 4)
    
    nlMonth <- substr(nlYearMonth, 5, 6)
    
    nlTile <- tileName2Idx(tile)
    
    print(paste0(nlYear, nlMonth, nlTile))
    
    #download tile
    success <- success && getNtLtsViirs(nlYear, nlMonth, nlTile)
  }
  
  return (success)
}

getAllNlYearMonthsTiles <- function(nlYearMonths, tileList)
{
  for (nlYearMonth in nlYearMonths)
  {
    getNlYearMonthTilesVIIRS(nlYearMonth, tileList)
  }
}

getCtryCodeTileList <- function(ctryCode)
{
  if (is.null(ctryCodeTiles))
    ctryCodeTiles <<- mapAllCtryPolyToTiles()
  
  ctryTiles <- unlist(ctryCodeTiles[which(ctryCodeTiles$code == ctryCode), "tiles"])
  
  return (ctryTiles)
}

processNtLts <- function (ctryCodes, nlYearMonths)
{
  #nlYearMonths is a character vector with each entry containing an entry of the form YYYYMM (%Y%m)
  #e.g. 201401 representing the month for which nightlights should be calculated
  #use provided list
  #if none given default to all year_months
  #TODO:
  #1. if years only given, infer months
  #2.verification & deduplication
  
  if (length(nlYearMonths) == 0)
  {
    nlYears <- getAllNlYears()
  }

  #use supplied list of ctryCodes in ISO3 format else use default of all
  #TODO: 
  #1.accept other formats and convert as necessary
  #2.verification & deduplication
  if (length(ctryCodes) == 0)
  {
    #get list of all country codes
    ctryCodes <- getAllNlCtryCodes()
  }
  
  ##First step: Determine which tiles are required for processing. This is determined by the 
  #list of ctryCodes. Since theoretically we need the polygons of all countries to determine which
  #tiles to download we take the opportunity to download all required shapefiles
  #Practically, we can stop as soon as all 6 tiles are flagged for download.
  
  ##If any tiles cannot be found/downloaded then abort and try for next country
  #we probably need to flag failed downloads so we don't try to process them and report back to user
  
  for (ctryCode in ctryCodes)
  {
    dnldCtryPoly(ctryCode)
  }
  
  #for all nlYearMonths check if the tiles exist else download
  for (nlYearMonth in nlYearMonths)
  {
    nlType <- getNlType(substr(nlYearMonth,1,4))

    #init the list of tiles to be downloaded
    tileList <- NULL
    
    #determine tiles to download
    if (nlType == "VIIRS")
    {
      #For each country
      for (ctryCode in unique(ctryCodes))
      {
        ctryTiles <- unlist(ctryCodeTiles[which(ctryCodeTiles$code == ctryCode), "tiles"])
        
        tileList <- c(tileList, setdiff(ctryTiles, tileList))
      }
    }
    else if(nlType == "OLS")
    {
      print("tile list not required")
    }
    else
    {
      return ("Unknown nlType")
    }
    
    #download the tiles
    if (nlType == "VIIRS")
    {
      if (!getNlYearMonthTilesVIIRS(nlYearMonth, tileList))
      {
        print("Something went wrong with the tile downloads. Aborting ...")
        
        break
      }
      
      #for all required countries
      for (ctryCode in unique(ctryCodes))
      {
        processNLCountryVIIRS(ctryCode, nlYearMonth)
      }
    }
    else if (nlType == "OLS")
    {
      if (!getNlYearMonthTilesOLS(nlYearMonth))
      {
        print("Something went wrong with the tile downloads. Aborting ...")
        
        break
      }
      
      #for all required countries
      for (ctryCode in unique(ctryCodes))
      {
        processNLCountryOls(ctryCode, nlYearMonth)
      }
      
    }
  }
}

initNtLts <- function()
{
  #the constructor
  
  #set directory paths (tiles, ctrypoly, output/cropped rasters, downloads/temp?)
  
  #create directories
  if(!dir.exists(dirPolygon))
    dir.create(dirPolygon)
  
  if(!dir.exists(dirRasterOLS))
    dir.create(dirRasterOLS)
  
  if(!dir.exists(dirRasterVIIRS))
    dir.create(dirRasterVIIRS)
  
  if(!dir.exists(dirNlData))
    dir.create(dirNlData)
  
  if(!dir.exists(dirRasterOutput))
    dir.create(dirRasterOutput)
  
  tSpPolysDFs <<- createNlTilesSpPolysDF()
  
  ctryCodeTiles <<- mapAllCtryPolyToTiles()
  #
}

cleanup <- function()
{
  #the destructor
  
  #del temp files
  
  #ensure files have been written that need to
  
  #
}